---
output:
  pdf_document:
    keep_tex: true
header-includes:
  - \usepackage[spanish,es-tabla]{babel}
---

---
documentclass: book
classoption: a4paper,oneside
title: "UNIVERSIDAD AUTÓNOMA DE NUEVO LEÓN \n &nbsp;  \n FACULTAD DE CIENCIAS FÍSICO MATEMÁTICAS \n &nbsp;  \n \n &nbsp;  \n  Maestria en Ciencia de Datos.  \n &nbsp;  \n Metodos Estadisticos Multivariados \n &nbsp;  \n Reporte Estadistico "
author: "MET.Rosa Isela Hernández Zamora  \n &nbsp;  \nAlumnos: \n Jesus Emmanuel Ramos Davila  \n Marco Antonio Obregon Flores \n &nbsp;  \nMatricula: 1439401, \n 1723556 \n"
date: "Fecha entrega: 03/28/2023"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
csl: biomed-central.csl
fontzise: 12pt
geometry: margin = 2.5cm

---


```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "", echo = TRUE)
#vignette('knit_print', package = 'knitr')

library(ggplot2)
library(corrplot)
library(readr)
set.seed(1234)
measures_v2 <- read_csv("measures_v2.csv", show_col_types = F)
measures_v2 <- measures_v2[,1:12]
datos<- measures_v2[sample(nrow(measures_v2), 500, replace = FALSE), ]
```

\newpage
\pagestyle{empty}
\tableofcontents

\newpage

## Introducción


El presente informe tiene como objetivo utilizar técnicas estadísticas multivariadas para analizar un conjunto de datos que contiene múltiples mediciones realizadas a un motor síncrono de imán permanente (PMSM). En específico, se utilizarán el análisis de componentes principales (PCA) y el análisis de factores para reducir la dimensionalidad del conjunto de datos y descubrir patrones y relaciones entre las variables. Estas técnicas se aplicarán en R Studio, utilizando los conocimientos adquiridos en el curso de Métodos Estadísticos Multivariados. El conjunto de datos contiene 13 variables distintas, incluyendo la temperatura del refrigerante del motor, la velocidad del motor, el par del motor y la corriente en el eje d del motor, entre otros. El análisis de componentes principales permitirá simplificar el conjunto de datos encontrando mezclas de variables que describen la mayor parte de la variación en los datos, mientras que el análisis de factores buscará reducir la dimensionalidad de los datos para explicar el máximo de información contenida en ellos. 

El análisis del motor síncrono de imán permanente es importante para entender su funcionamiento y optimizar su rendimiento. El objetivo del análisis multivariado es identificar patrones y relaciones entre las variables, lo que puede llevar a descubrir factores importantes que afectan el rendimiento del motor.Al reducir la dimensionalidad de los datos mediante el uso de técnicas como el análisis de componentes principales y el análisis de factores, se puede obtener una mejor comprensión de la estructura subyacente de los datos y reducir la complejidad de la información. Esto puede permitir una mejor visualización de los patrones y relaciones, lo que puede conducir a una mejor identificación de los factores clave que afectan el rendimiento del motor. Además, la reducción de la dimensionalidad también puede ayudar a simplificar el análisis y mejorar la eficiencia del procesamiento de datos.

Los datos utilizados en este análisis son reales y fueron obtenidos de Kirgiz, W. (2021). Electric Motor Temperature. Los datos se pueden encontrar en \url{https://www.kaggle.com/datasets/wkirgsn/electric-motor-temperature} y contienen mediciones realizadas a un motor síncrono de imán permanente (PMSM). Es importante destacar que no se eliminaron valores atípicos o faltantes en los datos.

Además, se estandarizaron los datos antes de realizar el análisis, lo que significa que se convirtieron todas las variables a la misma escala para que tengan una media de cero y una desviación estándar de uno. Esto se hizo para que las variables se puedan comparar directamente entre sí y para evitar que una variable tenga más peso en el análisis solo porque tiene valores más grandes.

En resumen, este informe utilizará técnicas estadísticas avanzadas para analizar un conjunto de datos complejo y encontrar patrones y relaciones entre las variables del motor síncrono de imán permanente.

\pagebreak


## Análisis descriptivo del conjunto de datos


Los registros corresponden a mediciones realizadas a un motor síncrono de imán permanente (PMSM), los cuales fueron muestreados a una frecuencia de 2 Hz. El conjunto de datos contiene múltiples sesiones de medición, las cuales se pueden distinguir por el identificador de perfil (profile\textunderscore id) y tienen una duración variable de entre una y seis horas. En total, se registraron 185 horas de operación del motor.

El dataset utilizado en este análisis contiene un total de 1,330,816 mediciones realizadas al motor síncrono de imán permanente. Este es un conjunto de datos bastante grande que requiere técnicas estadísticas multivariadas avanzadas para su análisis y comprensión. La cantidad de mediciones en este conjunto de datos proporciona una gran cantidad de información sobre el comportamiento del motor, lo que puede ser útil para identificar patrones y relaciones complejas entre las variables y optimizar su rendimiento.

\begin{table}[htbp]
\centering
\caption{Variables del conjunto de datos}
\label{tab:variables}
\begin{tabular}{c|c}
\textbf{Variable} & \textbf{Descripción} \\ \hline
\hline
u\textunderscore q & Tensión en el eje q del motor \\ \hline
coolant & Temperatura del refrigerante del motor \\ \hline
stator\textunderscore winding & Temperatura del devanado del estator del motor \\ \hline
u\textunderscore d & Tensión en el eje d del motor \\ \hline
stator\textunderscore tooth & Temperatura del diente del estator del motor \\ \hline
motor\textunderscore speed & Velocidad del motor \\ \hline
i\textunderscore d & Corriente en el eje d del motor \\ \hline
i\textunderscore q & Corriente en el eje q del motor \\ \hline
pm & Temperatura del imán permanente del motor \\ \hline
stator\textunderscore yoke & Temperatura del yugo del estator del motor \\ \hline
ambient & Temperatura ambiente durante la medición \\ \hline
torque & Par del motor \\ \hline
profile\textunderscore id & Identificador de la sesión de medición \\ \hline
\end{tabular}
\end{table}

Cabe destacar que el motor es excitado por ciclos de conducción diseñados a mano, que establecen una velocidad de referencia y un par de referencia. Las corrientes y voltajes en coordenadas d/q son resultado de una estrategia de control estándar que intenta seguir la velocidad y el par de referencia, y las variables de velocidad y torque son las cantidades resultantes logradas por esa estrategia, derivadas de las corrientes y voltajes establecidos. La mayoría de los ciclos de conducción corresponden a caminatas aleatorias en el plano velocidad-par, con el fin de imitar ciclos de conducción del mundo real de manera más precisa que las excitaciones y rampas de subida y bajada constantes.


### Análisis exploratorio

En esta sección se analizarán los histogramas y boxplot de las variables, para esta sección se incluirán pruebas de normal univariada y multivariada así como grafica de correlación de pearson a fin de encontrar cuales variables se relacionan más con otras. Para esta y demás secciones se omitirá una de las variables mostradas de la sección anterior la cual es **profile_id** la cual no es una medición de nuestros datos y solo identifica la observación.


#### Histogramas

**Histograma u_q (Tensión en el eje q del motor)**

```{r Hist_u_q, echo=F, warning=FALSE}
hist(datos$u_q, main="Histograma de Tensión en el eje q del motor",ylab = "Frecuencia", xlab = "u_q",  ylim = c(0,80), xlim = c(-10,150))
```
Fig 1: Histograma u_q

**Observaciones:** _podemos observar que no se muestra una curva como una distribución de tipo normal, asemeja más a una distribución de tipo uniforme._



**Histograma coolant (Temperatura del refrigerante del motor)**


```{r Hist_coolant, echo=F, warning=FALSE}
hist(datos$coolant, main="Histograma de temperatura del refrigerante del motor",ylab = "Frecuencia", xlab = "coolant",  ylim = c(0,250), xlim = c(0,110))
```
Fig 2: Histograma coolant

**Observaciones:** _podemos observar que no se muestra una curva como una distribución de tipo normal, asemeja más a una distribución de tipo exponencial._


**Histograma stator_winding (Temperatura del devanado del estator del motor)**

```{r Hist_stator_winding, echo=F, warning=FALSE}
hist(datos$stator_winding, main="Histograma de temperatura del devanado del estator del motor",ylab = "Frecuencia", xlab = "stator_winding", ylim = c(0,70), xlim = c(0,150))
```
Fig 3: Histograma stator_winding

**Observaciones:** _Se observa en la grafica que esta variable podria seguir una distribucion de tipo normal, de cualquier manera se realizaran pruebas de normalidad univariada a fin de observar cual variable sigue una distribucion de tipo normal._


**Histograma u_d (Tensión en el eje d del motor)**


```{r Hist_u_d, echo=F, warning=FALSE}
hist(datos$u_d, main="Histograma de Tensión en el eje d del motor",ylab = "Frecuencia", xlab = "u_d", ylim = c(0, 140))
```
Fig 4: Histograma u_d

**Observaciones:** _Podemos observar que no se muestra una curva como una distribución de tipo normal, se observa solo 2 barras con una gran cantidad de observaciones, se revisara en la sección de **vector de promedios** donde se ubican la media de los datos para esta variable._


**Histograma stator_tooth (Temperatura del diente del estator del motor)**


```{r Hist_stator_tooth, echo=F, warning=FALSE}
hist(datos$stator_tooth, main="Histograma de temperatura del diente del estator del motor",ylab = "Frecuencia", xlab = "stator_tooth", ylim = c(0,100))
```
Fig 5: Histograma stator_tooth

**Observaciones:** _Podemos observar que esta variable podría seguir una distribución de tipo normal dado los extremos y una forma un poco ligera de campana similar a la normal._



**Histograma motor_speed (Velocidad del motor)**

```{r Hist_motor_speed, echo=F, warning=FALSE}
hist(datos$motor_speed, main="Histograma de Velocidad del motor",ylab = "Frecuencia", xlab = "motor_speed", xlim = c(0, 6500), ylim = c(0,100))
```
Fig 6: Histograma motor_speed 


**Observaciones:** _Podemos observar que no se muestra una curva como una distribución de tipo normal._



**Histograma i_d (Corriente en el eje d del motor)**

```{r Hist_i_d, echo=F, warning=FALSE}
hist(datos$i_d, main="Histograma de corriente en el eje d del motor",ylab = "Frecuencia", xlab = "i_d")
```
Fig 7: Histograma i_d

**Observaciones:** _Podemos observar que esta variable no sigue una distribución normal , se asemeja más a una distribución de tipo exponencial._



**Histograma i_q (Corriente en el eje q del motor)**

```{r Hist_i_q, echo=F, warning=FALSE}
hist(datos$i_q, main="Histograma de corriente en el eje q del motor",ylab = "Frecuencia", xlab = "i_q")
```
Fig 8: Histograma i_q

**Observaciones:** _Podemos observar que esta variable no sigue una distribución normal._



**Histograma pm (Temperatura del imán permanente del motor)**


```{r Hist_pm, echo=F, warning=FALSE}
hist(datos$pm, main="Histograma de temperatura del imán permanente del motor",ylab = "Frecuencia", xlab = "pm", ylim = c(0,120), xlim = c(0,130))
```
Fig 9: Histograma pm

**Observaciones:** _Podemos observar que esta variable no sigue una distribución normal._


**Histograma ambient (Temperatura ambiente durante la medición)**


```{r Hist_ambient, echo=F, warning=FALSE}
hist(datos$ambient, main="Histograma de temperatura ambiente durante la medición",ylab = "Frecuencia", xlab = "ambient")
```
Fig 10: Histograma ambient

**Observaciones:** _Podemos observar que esta variable podría seguir una distribución de tipo normal dado los extremos._



**Histograma stator_yoke (Temperatura del yugo del estator del motor)**


```{r Hist_stator_yoke, echo=F, warning=FALSE}
hist(datos$stator_yoke, main="Histograma de temperatura del yugo del estator del motor",ylab = "Frecuencia", xlab = "stator_yoke")
```
Fig 11: Histograma stator_yoke

**Observaciones:** _Podemos observar que esta variable podría seguir una distribución de tipo normal dado los extremos._



**Histograma torque (Par del motor)**


```{r Hist_torque, echo=F, warning=FALSE}
hist(datos$torque, main="Histograma de par del motor",ylab = "Frecuencia", xlab = "torque")
```
Fig 12: Histograma torque

**Observaciones:** _Podemos observar que esta variable podría seguir una distribución de tipo normal dado los extremos._




#### Vector de Promedios

Antes de realizar una estandarizacion de los datos, procederemos a obtener el vector de promedios de cada una de las variables

\begin{table}[htbp]
\centering
\caption{Medias del conjunto de datos}
\label{tab:variables}
\begin{tabular}{c|c}
\textbf{Variable} & \textbf{Promedio} \\ \hline
\hline
u\textunderscore q & 55.6 \\ \hline
coolant & 35.5 \\ \hline
stator\textunderscore winding & 64.7 \\ \hline
u\textunderscore d & -27.5 \\ \hline
stator\textunderscore tooth & 55.7 \\ \hline
motor\textunderscore speed & 2254.6 \\ \hline
i\textunderscore d & -69.5 \\ \hline
i\textunderscore q & 40.3 \\ \hline
pm & 57.8 \\ \hline
stator\textunderscore yoke & 47.2 \\ \hline
ambient & 24.5 \\ \hline
torque & 33.4 \\ \hline
\end{tabular}
\end{table}


**Observaciones:** _Se observa que la mayoría de los promedios de los datos se encuentran en un rango similar a excepción de la variable **motor_speed** la cual hace sentido ya que es la velocidad del motor._



```{r vec_prom, echo=F, warning=F}
x_bar <- sapply(datos, mean)

```


#### Matriz de Correlaciones


```{r EstDatosGeneral, echo=F, warning=F}
standardize = function(x){
  z <- (x - mean(x)) / sd(x)
  return( z)
}

datos.estd <- apply(datos ,2, standardize)
```

```{r corPlotFactores, echo=F, warning=F}
datos.cor <- round(cor(datos.estd, method = "pearson"), digits = 2)
corrplot(datos.cor, method = "circle", addCoef.col = "blue", 
         order = "original", type = "upper", number.cex = 0.5, tl.cex = 0.8)
```
Fig 13: Plot correlaciones

**Observaciones:** Se observa fuertes correlaciones tanto positivas como negativas. Las correlaciones mas notables mostradas en la gráfica son:



\begin{table}[htbp]
\centering
\caption{Variables con mayor correlacion}
\label{tab:variables}
\begin{tabular}{c|c}
\textbf{Relacion} & \textbf{Coeficiente} \\ \hline
\hline
coolant - stator\textunderscore tooth & 0.67 \\ \hline
stator\textunderscore winding - stator\textunderscore tooth & 0.97 \\ \hline
stator\textunderscore winding - i\textunderscore d & -0.63 \\ \hline
u\textunderscore q - motor\textunderscore speed & 0.62 \\ \hline
u\textunderscore d - i\textunderscore q & -0.73 \\ \hline
motor\textunderscore speed - i\textunderscore d & -0.71 \\ \hline
stator\textunderscore tooth - pm & 0.86 \\ \hline
stator\textunderscore winding - pm & 0.83 \\ \hline
coolant - stator\textunderscore yoke  & 0.86 \\ \hline
stator\textunderscore winding - stator\textunderscore yoke  & 0.86 \\ \hline
stator\textunderscore tooth - stator\textunderscore yoke & 0.95 \\ \hline
pm - stator\textunderscore yoke & 0.78 \\ \hline
coolant - ambient & 0.59 \\ \hline
stator\textunderscore tooth - stator\textunderscore yoke &  0.95 \\ \hline
pm - ambient & 0.56\\ \hline
u\textunderscore d - torque & -0.76 \\ \hline
i\textunderscore q - torque & 1\\ \hline
\end{tabular}
\end{table}


**Observaciones:** Se observa una cantidad de fuertes correlaciones arriba de 0.70, tanto negativas como positivas. Una de las correlaciones más notorias es una correlación perfecta entre la variable **i_q y torque** las cual es de 1.


#### Prueba de Normalidad

Para esta sección haremos uso de la librería en R `nortest`, la cual aplicaremos para nuestra prueba de normalidad tanto univariada como multivariada.
Aunque nuestros datos provienen de una muestra grande, la cual es considerada como n-p > 40 la cual se igual a  500 – 13  > 50 , se concluye muestra grande. Se aplicara  de igual manera la prueba de normal multivariada.


```{r normMultiQQPLOT, echo=F, warning=F}
library(MVN)
library(nortest)
mvn(datos, mvnTest = "royston", univariateTest = "CVM", univariatePlot = "histogram",
multivariatePlot = "qq", multivariateOutlierMethod = "adj",
showOutliers = FALSE, showNewData = FALSE)
```
Fig 14: QQplot
Fig 15: Outliers

Establecemos las hipótesis a considerar para normal multivariada y univariada.

Ho: Los datos provienen de una normal multivariada H1: Los datos no provienen de una normal multivariada.


Ho: La variable proviene de una distribución normal, H1: La variable sigue otro tipo de distribución.


**Observaciones:** Con respecto a la prueba de normal multivariada se establece que se rechaza Ho si p-valor (**0.00000000157**) es menor a alfa 0.05 por lo tanto se rechaza Ho, los datos **no provienen de una distribución normal multivariada**.
Para las pruebas de normal univariada se establece que se rechaza Ho si p-valor es menor a alfa 0.05, Se rechaza Ho para todas las variables, **ninguna de las variables cumplió la normal univariada**.
Por otra parte la grafica de outliers muestra la mayoría de los puntos como **outliers los cuales fueron 244/500** observaciones  los cuales son casi la mitad de las observaciones ,con respecto a la gráfica de QQplot se observa que las observaciones al inicio se ajustaron a la línea de la normal pero la mayoría de las observaciones no pudieron ajustarse a la línea.



### Análisis de Factores

El Análisis Factorial es, por tanto, una técnica de reducción de la dimensionalidad de los datos. Su
propósito último consiste en buscar el número mínimo de dimensiones capaces de explicar el máximo
de información contenida en los datos.

Para desarrollar el análisis de factores se realizaran pasos previos tales como estandarizar los datos, verificar si los datos cumplen la **normal multivariada**, revisar la **matriz de correlaciones** y realizar **supuestos e hipótesis**.

#### Paso 1: Carga de Datos


```{r cargaInicialDatosFactores, echo=FALSE, warning=FALSE}

knitr::knit_print(measures_v2)
```
#### Paso 2: Estandarizar datos

```{r estandarizarDatosMeasure, echo=FALSE, warning=FALSE}
standardize = function(x){
  z <- (x - mean(x)) / sd(x)
  return( z)
}


datos.estd <- apply(datos ,2, standardize)
head(datos.estd)
```




#### Paso 3: Revisar de cumplimiento de normal multivariada

Para este cumplimiento de normal multivarida creamos nuestras hipótesis:



\[H_0 : \mu_1 = \mu_2 = \mu_3 ... \mu_k \]
\[H_1 : \mu_1 \neq \mu_2 ...  \neq \mu_k\]

```{r NormalMultivaridaFactores, echo=FALSE, warning=F}
library(MVN)
mvn.d <-mvn(datos.estd) 
mvn.d$multivariateNormality
```

Para el cumplimiento de normal univariada creamos de igual manera nuestras hipótesis:

**Ho** : los datos provienen de una distribución normal. \

**H1** : los datos provienen de otra distribución.


```{r NormalUnivarida, echo=FALSE, warning=F}
mvn.d$univariateNormality
```
**Observaciones:** Se observa que no se cumplio con la prueba de normal multivariada dado su _p-valor_ es **0** menor a alfa **0.05**, re rechaza **Ho** los datos **no provienen de una normal multivariada**, con respecto a las pruebas de **normalidad univariada** se observa que **ninguna variable** cumplio con normalidad dados sus _p-valores_ cercanos al cero y menores a alfa **0.05** por lo tanto los datos siguen otro tipo de distribución.



**Matriz de Correlaciones**

Análisis incluido en la sección `Análisis exploratorio` - _subsección: Matriz de Correlaciones_


#### Paso 4: Prueba de esfericidad

Para esta prueba se usara la prueba de esfericidad de Bartlett la cual sirve para identificar si la correlación entre pares de variables es cero o no.

Definimos nuestras hipótesis

Ho: La correlación entre cada par de variables es cero
H1: La correlación entre cada par de variable diferente de cero



```{r bartlettFactores, warning=F, echo=F, message=FALSE}
library(psych)
correlaciones <- corr.test(datos.estd)
R <- as.matrix(correlaciones$r)
cortest.bartlett(R, n=100)
```

**Observaciones:** Dado que el _p_valor_ es menor a alfa **0.05** , se rechaza Ho por lo tanto las correlaciones son diferente de 0.

#### Paso 5: Determinar numero de factores

##### Prueba de PCA

Para determinar el número de factores, procederemos a realizar un Análisis de Componentes Principales (PCA), el cual nos sugerirá el número de factores a considerar.

```{r PCAFactoresSinRotar, echo=F, warning=F, message=F}
com_principales <- prcomp(datos.estd)
summary(com_principales) # proporci?n de varianza explicada por cada componente
```

**Observaciones:** Se puede observar que el número de factores óptimo esta entre 3 y 4, dada la varianza acumulada que contienen.


##### Grafica de Codo

Creamos nuestra gráfica de codo la cual no sugiere de igual manera elegir entre 3 y 4 factores.

```{r NumeroFactorePCA, echo=F, warning=F}
fa.parallel(R, fm = "pa", n.obs = 100, ylabel = "Eigenvalues")
```
Fig 16: Gráfica de Codo


**Observaciones:** Se puede observar que al elegir 3 factores obtenemos **82%** de la varianza explicada, la cual es un buen porcentaje, Procedemos a usar el algoritmo ahora rotando los ejes usando el método de 'varimax'.


##### Factores usando Varimax

```{r PCAFactoresVarimax, echo=F, warning=F}
acp_fa <- principal(R, nfactors = 3, rotate = "varimax")
acp_fa
```

**Observaciones:** Se observa una varianza acumulada del **83%**, con respecto a los **residuales RSMR** se observa un valor muy bajo de **0.07** cercano a cero. Con respecto a las cargas elegidas estas muestran comunalidades (\[h_2\]) altas y la varianza no explicada \[u_2\] es muy baja. También observamos con el método de _varimax_ de una manera muy clara los **variables dominantes para cada factor** los cuales son:

- **Factor 1** : stator_winding, stator_tooth, pm, stator_yoke, coolant, ambient
- **Factor 2** : u_d,i_q,torque
- **Factor 3** : u_q,motor_speed,i_d,


#### Paso 6: Representación gráfica

Representación grafica de cada uno de las variables.

```{r PCAGrafFactores, echo=F, warning=F}
plot(acp_fa, labels = row.names(R))
```
Fig 17: Grafica Factores

**Observaciones:** Se puede observar una agrupación muy notoria en las variables "stator_winding, stator_tooth, pm, stator_yoke, coolant, ambient",  mientras que motor_speed y u_q están cercanas entre ellas, también se observa que las variable torque y u_d están muy cercanas, la única variable que está muy alejada de los grupos antes mencionados es la variable i_d.  

#### Conclusiones

Se concluye que aunque no se cumplieron los supuesto de normal multivariada dadas las pruebas de hipótesis, se obtuvo una varianza acumulada de 82% usando 3 factores con lo cual se redujo la dimensión de variables de 12 variables a solo 3, Por otra estos factores mostraron **comunalidades  muy altas** y **varianza no explicada muy baja**,  Con respecto a las variables dominantes de cada factor estas quedaron de la siguiente forma:

- **Factor 1** : stator_winding, stator_tooth, pm, stator_yoke, coolant, ambient
- **Factor 2** : u_d,i_q,torque
- **Factor 3** : u_q,motor_speed,i_d

**Nota**: Dados las variables dominantes para cada factor daremos un nombre que haga sentido a los factores.

- **Factor 1** : '**Componentes del motor**'
- **Factor 2** : '**Variables eléctricas y de torque**'
- **Factor 3** : '**Variables de velocidad y corriente directa**'

### Análisis de Componentes Principales

El análisis de componentes principales (PCA) es un método estadístico que sirve para simplificar y resumir un conjunto de datos con muchas variables. En vez de usar todas las variables originales, el PCA encuentra mezclas de estas variables que capturan la mayor parte de la variación en los datos. Estas mezclas se llaman “componentes principales” y se usan para describir el conjunto de datos de una manera más sencilla y comprensible.

Piensa que tienes muchos datos distintos sobre un motor eléctrico, como la temperatura, el torque, la velocidad, etc. Puede ser complicado saber cómo todos estos datos se relacionan entre sí y qué información es la más relevante. Si el análisis de componentes principales descubre que la temperatura y el torque están muy relacionados, entonces puede juntarlos en un nuevo componente principal que abarque ambas variables a la vez. Así, se puede disminuir el número de variables y hacer que los datos sean más sencillos de entender.

####  Paso 1: Carga de datos y preprocesamiento

En este primer paso se procede a la carga de los datos, los cuales deben ser preprocesados para asegurar que el análisis posterior sea correcto. En el código se utiliza la función read.csv para leer el archivo de datos measures_v2.csv, y se utiliza la librería dplyr para eliminar la columna profile_id, que contiene identificadores únicos para cada observación y no aporta información relevante para el análisis.


```{r, echo=FALSE, warning=F, message=FALSE}
datos_PCA <- read.csv("measures_v2.csv")
library(dplyr)
df_PCA <- as.data.frame(datos_PCA) #convertir std_data a dataframe 
df_PCA <- df_PCA %>% select(-profile_id) # eliminar la columna profile id

head(df_PCA)
```


#### Paso 2: Estandarización de datos

Antes de realizar el análisis de componentes principales, es necesario estandarizar los datos. En este paso se utiliza la función fn_std para estandarizar los datos de cada variable, es decir, se resta la media y se divide por la desviación estándar. Luego se aplica esta función a cada columna del dataframe df utilizando la función apply, y se muestran los primeros registros de los datos estandarizados.

```{r, echo = FALSE, warning=F}
# Estandarizar los datos
fn_std = function(x){
  z <- (x - mean(x)) / sd(x)
  return( z)
}
df_std_PCA <- apply(df_PCA,2, fn_std)
head(df_PCA)

```

De igual manera se validan las dimensiones del dataset con dim().

```{r, echo = FALSE, warning=F, message=FALSE}
  dim_table <- as.data.frame(t(dim(df_std_PCA)))
  colnames(dim_table) <- c("Filas", "Columnas")
  library(knitr)
  kable(dim_table, caption = "Tabla de dimensiones del dataset") 
```
####  Paso 3: Análisis de componentes principales

El análisis de componentes principales (PCA) se utiliza para identificar patrones en los datos y reducir la dimensionalidad de los mismos. En este paso se utiliza la función prcomp para realizar el análisis de componentes principales sobre los datos estandarizados.

```{r, echo = FALSE, warning=F}
# Análisis de componentes principales
pca1 <- prcomp(df_std_PCA, scale = FALSE)
pca1
```

Estas desviaciones estándar indican cuánto varían los datos proyectados en cada componente principal. A mayor desviación estándar, mayor variación y mayor importancia del componente principal.

Se puede observar que el primer componente principal tiene la mayor desviación estándar (2.2), lo que significa que explica la mayor parte de la variación de los datos originales. El segundo componente principal tiene la segunda mayor desviación estándar (1.77), lo que significa que explica la mayor parte de la variación que queda después de quitar el efecto del primero. Y así sucesivamente con los demás componentes principales.

Se puede ver también que las desviaciones estándar van disminuyendo a medida que aumenta el número de componente principal, lo que significa que cada componente principal explica menos variación que el anterior. Los últimos componentes principales tienen desviaciones estándar muy pequeñas (menores a 0.1), lo que significa que explican muy poca variación de los datos y que podrían ser ignorados sin perder mucha información.

Sobre los resultados de la matriz de rotación y los coeficientes de los componentes principales obtenidos por el PCA, la matriz de rotación es una matriz que contiene los vectores propios de los datos, es decir, las direcciones en las que los datos varían más. Cada fila de la matriz representa un vector propio y cada columna representa una coordenada del vector. Los coeficientes de los componentes principales son los números que indican cómo se combinan las variables originales para formar cada componente principal. Cada fila de la tabla representa una variable original y cada columna representa un componente principal.

Los números pueden ser positivos o negativos. Los números positivos indican que la variable original y el primer componente principal se mueven en la misma dirección. Por ejemplo, si la variable coolant aumenta, el primer componente principal también aumenta. Los números negativos indican que la variable original y el primer componente principal se mueven en direcciones opuestas. Por ejemplo, si la variable u_d aumenta, el primer componente principal disminuye.

Los números grandes indican que la variable original tiene mucha influencia en el primer componente principal. Por ejemplo, la variable stator_yoke tiene el número más grande (0.42185478), lo que significa que es la que más contribuye a la diferencia entre los datos. Los números pequeños indican que la variable original tiene poca influencia en el primer componente principal. Por ejemplo, la variable torque tiene el número más pequeño (-0.05489022), lo que significa que es la que menos contribuye a la diferencia entre los datos.

El primer componente principal es importante porque explica la mayor parte de la diferencia entre los datos. 
El doceavo componente principal es el último que se puede hacer con las variables originales. Es el que explica la menor parte de la diferencia entre los datos

Támbien, se calculan los **eigenvalores** como el cuadrado de las desviaciones estándar de las componentes principales, que se encuentran en el objeto pca1$sdev.

```{r, echo = FALSE, warning=F}
eigenvalues <- pca1$sdev^2
kable(data.frame(Eigenvalue = eigenvalues), 
      caption = "Eigenvalores del PCA")
```
La interpretación de los eigenvalores es que representan la varianza explicada por cada una de las componentes principales. En este caso, el primer eigenvalor es 4.87, lo que indica que la primera componente principal explica el 48.7% de la varianza total de los datos originales. El segundo eigenvalor es 3.15, lo que representa el 31.5% de la varianza total, y así sucesivamente para cada uno de los eigenvalores.

#### Paso 4: Selección de componentes principales

En este paso se seleccionan los componentes principales que explican más del 80% de la varianza en los datos. Primero se utiliza la función cumsum para calcular la suma acumulativa de las varianzas explicadas por cada componente principal, y luego se determina el número de componentes necesarios para alcanzar el 80% de la varianza explicada. En este caso, se obtiene que se necesitan 3 componentes principales.

```{r, echo = FALSE, warning=F}
suma1 <- cumsum(pca1$sdev^2 / sum(pca1$sdev^2))
suma1_table <- data.frame(Componente = 1:length(suma1), Varianza_acumulada = suma1)
kable(suma1_table, caption = "Tabla de varianza acumulada explicada por cada componente principal")
num_componentes1 <- length(suma1[suma1 <= 0.82])


```

De igual manera, se crea una gráfica de codo utilizando la tabla que muestra la varianza acumulada explicada por cada componente principal. La línea muestra cómo la varianza acumulada explicada aumenta a medida que se incluyen más componentes principales. La línea vertical punteada indica el número de componentes necesarios para alcanzar una varianza acumulada del 82%, como se determinó previamente.

```{r, echo = FALSE, warning=F, message=FALSE}
library(ggplot2)
ggplot(suma1_table, aes(x = Componente, y = Varianza_acumulada)) + 
  geom_line() + 
  scale_x_continuous(breaks = seq(1, length(suma1), by = 1)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +
  geom_vline(xintercept = num_componentes1, linetype = "dashed") +
  xlab("Número de Componentes") + 
  ylab("Varianza acumulada explicada") +
  ggtitle("Gráfico de codo del PCA") +
  theme_bw()
```

\newpage

#### Paso 5: Visualización de componentes principales

Finalmente, se utiliza la función pca1$rotation para obtener la matriz de rotación, que contiene las cargas de cada variable en cada componente principal, y se muestran los primeros 3 componentes principales y sus respectivas cargas.

```{r, echo=FALSE, warning=F}
kable(pca1$rotation[,1:3], caption = "Tabla de componentes principales")
```

Los valores en la tabla indican cómo cada variable contribuye a cada una de las tres componentes principales.

El primer componente principal tiene valores grandes en las variables coolant, stator_winding, stator_tooth, i_d, pm y stator_yoke. Esto significa que estas variables contribuyen mucho a la diferencia entre los datos en la dirección del primer componente principal y que están correlacionadas entre sí. Podríamos interpretar este componente principal como un factor relacionado con la temperatura del motor.

El segundo componente principal tiene valores grandes en las variables u_d, i_q y torque. Esto significa que estas variables contribuyen mucho a la diferencia entre los datos en la dirección del segundo componente principal y que están correlacionadas entre sí. Podríamos interpretar este componente principal como un factor relacionado con la potencia del motor.

El tercer componente principal tiene valores grandes en las variables u_q y motor_speed. Esto significa que estas variables contribuyen mucho a la diferencia entre los datos en la dirección del tercer componente principal y que están correlacionadas entre sí. Podríamos interpretar este componente principal como un factor relacionado con la velocidad del motor.

\newpage

## Conclusiones

En esta sección se detalla las conclusiones de cada una de las secciones principales , tanto del Análisis exploratorio como de los métodos aplicados (Análisis de Factores y PCA)


### Análisis exploratorio.
En este análisis se observaron que las variables sus histogramas  a manera de sospecha seguían una distribución de tipo normal, otras asemejaban otro tipo de distribución,  al realizar la prueba de normalidad tanto univarida como multivariada concluimos que las los datos no seguían una distribución normal multivariada y individualmente ninguna de las variables seguía una distribución de tipo normal dados sus p-valores para cada uno de las pruebas.
Con respecto a la grafica de QQplot y de outliers se observo que las observaciones para QQplot no se ajustaban a la línea y la parte de los outliers la mitad de las observaciones de nuestro conjunto de datos se clasificaron como outliers.

### Análisis de Factores
En esta sección se reviso la matriz de correlaciones la cual se observaron correlación muy altas tanto positivas como negativas también se observaron variables independientes la cual su correlación entre variable fue 0,  Con respecto a la prueba de esfericidad se establecieron las hipótesis correspondientes y se aplico la prueba la cual se concluyo que las correlación entre las variables son diferentes de 0 dado su p-valor.
Para la parte de el numero optimo de factores se aplico una grafica de codo la cual observamos que el numero optimo de factores se encontraba entre 3 y 4 factores, al aplicar el algoritmo de factores usando varimax para la rotación de vectores, se obtuvo una varianza explicada del **82%** la cual es un buen porcentaje, con respecto a los residuales RSMR los valores fueron muy bajos cercanos a 0, se mostraron comunalidades alta y varianza explicada muy baja.  **Concluimos para este método que pudimos reducir nuestro numero de variables de 12 a solo 3 variables manteniendo un porcentaje muy bueno de varianza explicada, se revisó de acuerdo a la variables dominantes de cada uno de los factores y se dieron nombres que dieran sentido a estas nuevas variables**.

### Análisis de Componentes Principales

Los resultados obtenidos con el PCA nos pueden servir para realizar una investigación más avanzada o aplicar modelos no paramétricos o de deep learning al conjunto de datos. Al reducir el número de variables de 12 a 3, podemos disminuir la complejidad y el tiempo de cómputo de los modelos, así como evitar el sobreajuste o la maldición de la dimensionalidad. Además, al tener variables más simples y significativas, podemos facilitar la interpretación y la visualización de los resultados.

Por ejemplo, podríamos usar los tres componentes principales como variables predictoras en un modelo no paramétrico como un árbol de decisión o un bosque aleatorio para predecir el consumo eléctrico del motor en función de las características del mismo. O podríamos usar los tres componentes principales como variables de entrada en un modelo de deep learning como una red neuronal artificial o una red convolucional para clasificar el tipo o el estado del motor según su consumo eléctrico.

\newpage

## Referencias

- Jolliffe, I. T. (2002). Principal component analysis. Springer.

- Abdi, H., & Williams, L. J. (2010). Principal component analysis. Wiley Interdisciplinary Reviews: Computational Statistics, 2(4), 433–459. https://doi.org/10.1002/wics.101

- Jun Li and Thangarajah Akilan. Global attention-based encoder-decoder
lstm model for temperature prediction of permanent magnet synchronous
motors. IEEE, 11, 2022.

- Kirgiz, W. (2021). Electric Motor Temperature. Recuperado el 10 de enero de 2023, de \url{https://www.kaggle.com/datasets/wkirgsn/electric-motor-temperature}

- Wickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media.

- Kassambara, A. (2017). QQ-plots: Quantile-Quantile plots - R Base Graphs. STHDA. http://sthda.com/english/wiki/qq-plots-quantile-quantile-plots-r-base-graphs

- Lever, J., Krzywinski, M., & Altman, N. (2017). Principal component analysis. Nature Methods, 14(7), 641–642. https://doi.org/10.1038/nmeth.4346

- Revelle, W. (2018). How to: Use the psych package for factor analysis and data reduction. https://cran.r-project.org/web/packages/psychTools/vignettes/factor.pdf

- Kassambara, A. (2019). Mauchly’s test of sphericity in R: The definitive guide. Datanovia. https://www.datanovia.com/en/lessons/mauchlys-test-of-sphericity-in-r/

- Statology. (2019). A guide to Bartlett’s test of sphericity. https://www.statology.org/bartletts-test-of-sphericity/

- Singmann, H., Bolker, B., Westfall, J., & Aust, F. (2020). afex: Analysis of factorial experiments. https://cran.r-project.org/web/packages/afex/vignettes/assumptions_of_ANOVAs.html


