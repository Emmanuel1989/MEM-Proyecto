
---
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

---
documentclass: book
classoption: a4paper,oneside
title: "UNIVERSIDAD AUTÓNOMA DE NUEVO LEÓN \n &nbsp;  \n FACULTAD DE CIENCIAS FÍSICO MATEMÁTICAS \n &nbsp;  \n \n &nbsp;  \n  Maestria en Ciencia de Datos.  \n &nbsp;  \n Metodos Estadisticos Multivariados \n &nbsp;  \n Reporte Estadistico "
author: "MET.Rosa Isela Hernández Zamora  \n &nbsp;  \nAlumnos: \n Jesus Emmanuel Ramos Davila  \n Marco Antonio Obregon Flores \n &nbsp;  \nMatricula: 1439401"
date: "Fecha entrega: 03/28/2023"
output:
  bookdown::html_document2: default
bibliography: bibliography.bib
csl: biomed-central.csl
fontzise: 12pt
geometry: margin = 2.5cm

---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "", echo = TRUE)
#vignette('knit_print', package = 'knitr')

library(ggplot2)
library(corrplot)
```


## Indice

## Introducción

## Análisis descriptivo del conjunto de datos




## Análisis de Componentes Principales/ Factores/ Discriminante/ Conglomerado




### Análisis de Factores

El Análisis Factorial es, por tanto, una técnica de reducción de la dimensionalidad de los datos. Su
propósito último consiste en buscar el número mínimo de dimensiones capaces de explicar el máximo
de información contenida en los datos.

Para desarrollar el análisis de factores se realizaran pasos previos tales como estandarizar los datos , verificar si los datos cumplen la **normal multivariada**, revisar la **matriz de correlaciones** y realizar **supuestos e hipótesis**.

#### Paso 1: Carga de Datos


```{r cargaInicialDatosFactores, echo=FALSE, warning=FALSE}
library(readr)
set.seed(1234)
measures_v2 <- read_csv("measures_v2.csv", show_col_types = F)
measures_v2 <- measures_v2[,1:12]
datos<- measures_v2[sample(nrow(measures_v2), 500, replace = FALSE), ]
knitr::knit_print(measures_v2)
```
#### Paso 2: Estandarizar datos

```{r estandarizarDatosMeasure, echo=FALSE, warning=FALSE}
standardize = function(x){
  z <- (x - mean(x)) / sd(x)
  return( z)
}


datos.estd <- apply(datos ,2, standardize)
head(datos.estd)
```




#### Paso 3: Revisar de cumplimiento de normal multivariada

Para este cumplimiento de normal multivarida creamos nuestras hipótesis



\[H_0 : \mu_1 = \mu_2 = \mu_3 ... \mu_k \]
\[H_1 : \mu_1 \neq \mu_2 ...  \neq \mu_k\]

```{r NormalMultivaridaFactores, echo=FALSE, warning=F}
library(MVN)
mvn.d <-mvn(datos.estd) 
mvn.d$multivariateNormality
```

Para el cumplimiento de normal univariada creamos de igual manera nuestras hipótesis

**Ho** : los datos provienen de una distribución normal. \

**H1** : los datos provienen de otra distribución.


```{r NormalUnivarida, echo=FALSE, warning=F}
mvn.d$univariateNormality
```
**Observaciones:** Se observa que no se cumplio con la prueba de normal multivariada dado su _p-valor_ es **0** menor a alfa **0.05**, re rechaza **Ho** los datos **no provienen de una normal multivariada**, con respecto a las pruebas de **normalidad univariada** se observa que ninguna variable cumplio con normalidad dados sus _p-valores_ cercanos al cero y menores a alfa **0.05** por lo tanto los datos siguen otro tipo de distribución.


#### Paso 4: Grafica de correlaciones


```{r corPlotFactores, echo=F, warning=F}
datos.cor <- round(cor(datos.estd, method = "pearson"), digits = 2)
corrplot(datos.cor, method = "circle", addCoef.col = "blue", 
         order = "original", type = "upper", number.cex = 0.5, tl.cex = 0.8)
```


**Observaciones:** Se observa fuertes correlaciones tanto positivas como negativas, Las correlaciones mas notables mostradas en la grafica son:



| Relacion | Coeficiente |
| ------ | ------ |
| coolant & stator_tooth | 0.67 |
| stator_winding & stator_tooth | 0.97 |
| stator_winding & i_d | -0.63  |
| u_q & motor_speed | 0.62  |
| u_d & i_q | -0.73  |
| motor_speed & i_d | -0.71  |
| stator_tooth & pm | 0.86  |
| stator_winding & pm | 0.83  |
| coolant & stator_yoke | 0.86  |
| stator_winding & stator_yoke | 0.86  |
| stator_tooth & stator_yoke | 0.95  |
| pm & stator_yoke | 0.78  |
| coolant & ambient | 0.59  |
| stator_tooth & stator_yoke | 0.95 |
| pm & ambient | 0.56 |
| u_d & torque | -0.76 |
| i_q & torque | 1 |



**Observaciones:** Se observa una cantidad de fuertes correlaciones arriba de 0.70, tanto negativas como positivas. Una de las correlaciones más notorias es una correlación perfecta entre la variable **i_q y torque** las cual es de 1.


#### Paso 5: Prueba de esfericidad

Para esta prueba se usara la prueba de esfericidad de bartlett la cual sirve para identificar si la correlación entre pares de variables es cero o no.

Definimos nuestras hipótesis

Ho: La correlación entre cada par de variables es cero
H1: La correlación entre cada par de variable diferente de cero



```{r bartlettFactores, warning=F, echo=F}
library(psych)
correlaciones <- corr.test(datos.estd)
R <- as.matrix(correlaciones$r)
cortest.bartlett(R, n=100)
```

**Observaciones:** Dado que el _p_valor_ es menor a alfa **0.05** , se rechaza Ho por lo tanto las correlaciones son diferente de 0.

#### Paso 6: Determinar numero de factores


Para determinar el número de factores, procederemos a realizar un Análisis de Componentes Principales (PCA), el cual nos sugerirá el número de factores a considerar.


```{r NumeroFactorePCA, echo=F, warning=F}
fa.parallel(R, fm = "pa", n.obs = 100, ylabel = "Eigenvalues")
```

**Observaciones:** Se puede observar que el numero factores optimo esta entre 3 y 4, Procedemos a obtener un resumen del análisis PCA para revisar cuanta varianza explicada es la que se tiene cuando se toman 3 o 4 componentes.


```{r PCAFactores, echo=F, warning=F}
com_principales <- prcomp(datos.estd)
summary(com_principales) 
```
**Observaciones:** Se puede observar que al elegir 3 factores obtenemos 82% de la varianza explicada, la cual es un buen porcentaje, Procedemos a usar el algoritmo ahora rotando los ejes usando el metodo de 'varimax'.

```{r PCAFactoresVarimax, echo=F, warning=F}
acp_fa <- principal(R, nfactors = 3, rotate = "varimax")
acp_fa
```

**Observaciones:** Se observa una varianza acumulada del 83%, con respecto a los **residuales RSMR** se observa un valor muy bajo de **0.07** cercano a cero. Con respecto a las cargas elegidas estas muestran comunalidades (\[h_2\]) altas y la varianza no explicada \[u_2\] es muy baja. También observamos con el método de _varimax_ de una manera muy clara los **variables dominantes para cada factor** los cuales son:

- **Factor 1** : stator_winding, stator_tooth, pm, stator_yoke, coolant, ambient
- **Factor 2** : u_d,i_q,torque
- **Factor 3** : u_q,motor_speed,i_d,


#### Paso 7: Representación gráfica

Representacion grafica de cada uno de las variables.

```{r PCAGrafFactores, echo=F, warning=F}
plot(acp_fa, labels = row.names(R))
```

**Observaciones:** Se puede observar una agrupación muy notoria en las variables "stator_winding, stator_tooth, pm, stator_yoke, coolant, ambient",  mientras que motor_speed y u_q están cercanas entre ellas, también se observa que las variable torque y u_d están muy cercanas, la única variable que está muy alejada de los grupos antes mencionados es la variable i_d.  

#### Conclusiones

Se concluye que aunque no se cumplieron los supuesto de normal multivariada dadas las pruebas de hipótesis, se obtuvo una varianza acumulada de 82% usando 3 factores con lo cual se redujo la dimensión de variables de 12 variables a solo 3, Por otra estos factores mostraron **comunalidades  muy altas** y **varianza no explicada muy baja**,  Con respecto a las variables dominantes de cada factor estas quedaron de la siguiente forma:

- **Factor 1** : stator_winding, stator_tooth, pm, stator_yoke, coolant, ambient
- **Factor 2** : u_d,i_q,torque
- **Factor 3** : u_q,motor_speed,i_d,

## Conclusiones

## Referencias

